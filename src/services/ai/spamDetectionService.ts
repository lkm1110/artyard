/**
 * ğŸ¤– ê³ ê¸‰ ìŠ¤íŒ¸/ë„ë°° íƒì§€ AI ì‹œìŠ¤í…œ
 * 20ë…„ì°¨ ë¨¸ì‹ ëŸ¬ë‹ ê°œë°œìì˜ ì •êµí•œ ë©€í‹°ë ˆì´ì–´ íƒì§€ ì•Œê³ ë¦¬ì¦˜
 */

import { supabase } from '../supabase';
import CryptoJS from 'crypto-js';

export interface SpamDetectionResult {
  isSpam: boolean;
  confidence: number;
  spamType: string[];
  riskLevel: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL';
  recommendedAction: 'ALLOW' | 'SHADOW_BAN' | 'TEMP_BAN' | 'PERMANENT_BAN';
  details: {
    contentSimilarity: number;
    behaviorAnomalyScore: number;
    floodingScore: number;
    toxicityScore: number;
    botProbability: number;
  };
}

export interface ContentAnalysis {
  textHash: string;
  imageHash?: string;
  contentSignature: string;
  semanticVector: number[];
  metadata: {
    length: number;
    complexity: number;
    languagePattern: string;
    specialCharRatio: number;
  };
}

/**
 * ğŸ¯ 1ë‹¨ê³„: ì½˜í…ì¸  í•´ì‹œ ë° ì‹œê·¸ë‹ˆì²˜ ìƒì„±
 * - í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ì˜ ê³ ìœ  ì§€ë¬¸ ìƒì„±
 * - ë¯¸ì„¸í•œ ë³€í˜•ë„ íƒì§€ ê°€ëŠ¥í•œ í¼ì§€ í•´ì‹±
 */
class ContentHashingEngine {
  /**
   * í…ìŠ¤íŠ¸ì˜ ì‹œë§¨í‹± í•´ì‹œ ìƒì„± (ìœ ì‚¬í•œ ì˜ë¯¸ íƒì§€)
   */
  static generateTextHash(text: string): string {
    // ì •ê·œí™”: ì†Œë¬¸ì, ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
    const normalized = text
      .toLowerCase()
      .replace(/\s+/g, ' ')
      .replace(/[^\w\s]/g, '')
      .trim();
    
    return CryptoJS.SHA256(normalized).toString();
  }

  /**
   * í¼ì§€ í•´ì‹±ì„ í†µí•œ ìœ ì‚¬ í…ìŠ¤íŠ¸ íƒì§€
   */
  static generateFuzzyHash(text: string): string {
    const tokens = this.tokenizeText(text);
    const significantTokens = tokens
      .filter(token => token.length > 2)
      .sort()
      .slice(0, 20); // ìƒìœ„ 20ê°œ í† í°ë§Œ ì‚¬ìš©
    
    return CryptoJS.SHA256(significantTokens.join('|')).toString();
  }

  /**
   * ì´ë¯¸ì§€ ì¸ì‹ í•´ì‹œ (ì‹¤ì œ êµ¬í˜„ì‹œ Canvas/WebGL í•„ìš”)
   */
  static async generateImageHash(imageUrl: string): Promise<string> {
    try {
      // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” dhash, phash ë“± perceptual hashing ì‚¬ìš©
      // í˜„ì¬ëŠ” URL ê¸°ë°˜ ë‹¨ìˆœ í•´ì‹œë¡œ ëŒ€ì²´
      return CryptoJS.SHA256(imageUrl).toString();
    } catch (error) {
      console.error('ì´ë¯¸ì§€ í•´ì‹œ ìƒì„± ì‹¤íŒ¨:', error);
      return '';
    }
  }

  /**
   * í…ìŠ¤íŠ¸ í† í°í™” (ë‹¤êµ­ì–´ ì§€ì›)
   */
  private static tokenizeText(text: string): string[] {
    return text
      .toLowerCase()
      .replace(/[^\w\sê°€-í£]/g, ' ')
      .split(/\s+/)
      .filter(token => token.length > 0);
  }

  /**
   * ì½˜í…ì¸  ë³µì¡ë„ ë¶„ì„
   */
  static analyzeComplexity(text: string): number {
    const sentences = text.split(/[.!?]+/).length;
    const words = text.split(/\s+/).length;
    const uniqueWords = new Set(text.toLowerCase().split(/\s+/)).size;
    const avgWordLength = text.replace(/\s/g, '').length / Math.max(words, 1);
    
    return Math.min(1.0, (sentences * 0.1 + uniqueWords * 0.01 + avgWordLength * 0.05) / 3);
  }
}

/**
 * ğŸ¯ 2ë‹¨ê³„: í–‰ë™ íŒ¨í„´ ë¶„ì„ ì—”ì§„
 * - ì‚¬ìš©ìì˜ ì—…ë¡œë“œ/ëŒ“ê¸€ íŒ¨í„´ ë¶„ì„
 * - ì‹œê³„ì—´ ì´ìƒ íƒì§€
 */
class BehaviorAnalysisEngine {
  /**
   * í”ŒëŸ¬ë”©(ë„ë°°) ì ìˆ˜ ê³„ì‚°
   */
  static async calculateFloodingScore(userId: string, timeWindow: number = 3600): Promise<number> {
    const { data: recentBehaviors } = await supabase
      .from('user_behaviors')
      .select('behavior_type, timestamp, intensity_score')
      .eq('user_id', userId)
      .gte('timestamp', new Date(Date.now() - timeWindow * 1000).toISOString())
      .order('timestamp', { ascending: false });

    if (!recentBehaviors || recentBehaviors.length === 0) return 0;

    // ë¹ˆë„ ê¸°ë°˜ ì ìˆ˜
    const actionCounts = recentBehaviors.reduce((acc: any, behavior) => {
      acc[behavior.behavior_type] = (acc[behavior.behavior_type] || 0) + 1;
      return acc;
    }, {});

    // ë¹„ì •ìƒì  ë¹ˆë„ íƒì§€
    const maxNormalFreq = { upload: 5, comment: 20, like: 50 };
    let floodingScore = 0;

    for (const [actionType, count] of Object.entries(actionCounts)) {
      const normalLimit = maxNormalFreq[actionType as keyof typeof maxNormalFreq] || 10;
      if (count > normalLimit) {
        floodingScore += Math.min(1.0, (count - normalLimit) / normalLimit);
      }
    }

    return Math.min(1.0, floodingScore);
  }

  /**
   * ì‹œê°„ íŒ¨í„´ ì´ìƒ íƒì§€
   */
  static async detectTemporalAnomalies(userId: string): Promise<number> {
    const { data: behaviors } = await supabase
      .from('user_behaviors')
      .select('timestamp')
      .eq('user_id', userId)
      .order('timestamp', { ascending: false })
      .limit(100);

    if (!behaviors || behaviors.length < 10) return 0;

    // ì‹œê°„ ê°„ê²© ë¶„ì„
    const intervals = [];
    for (let i = 1; i < behaviors.length; i++) {
      const interval = new Date(behaviors[i-1].timestamp).getTime() - new Date(behaviors[i].timestamp).getTime();
      intervals.push(interval / 1000); // ì´ˆ ë‹¨ìœ„
    }

    // í‘œì¤€í¸ì°¨ ê³„ì‚°
    const mean = intervals.reduce((sum, interval) => sum + interval, 0) / intervals.length;
    const variance = intervals.reduce((sum, interval) => sum + Math.pow(interval - mean, 2), 0) / intervals.length;
    const stdDev = Math.sqrt(variance);

    // ë§¤ìš° ê·œì¹™ì ì¸ íŒ¨í„´ (ë´‡ ì˜ì‹¬)
    const regularityScore = stdDev < 5 ? 0.8 : Math.max(0, 0.5 - stdDev / 100);
    
    // ë§¤ìš° ì§§ì€ ê°„ê²©ì˜ ë¹ˆë„
    const rapidActions = intervals.filter(interval => interval < 2).length;
    const rapidScore = Math.min(1.0, rapidActions / 20);

    return Math.min(1.0, (regularityScore + rapidScore) / 2);
  }

  /**
   * ë´‡ íƒì§€ ì•Œê³ ë¦¬ì¦˜
   */
  static async calculateBotProbability(userId: string, userAgent: string = '', deviceFingerprint: string = ''): Promise<number> {
    let botScore = 0;

    // User-Agent ë¶„ì„
    const suspiciousAgents = ['bot', 'crawler', 'scraper', 'automated'];
    if (suspiciousAgents.some(agent => userAgent.toLowerCase().includes(agent))) {
      botScore += 0.6;
    }

    // í–‰ë™ íŒ¨í„´ ë¶„ì„
    const temporalScore = await this.detectTemporalAnomalies(userId);
    botScore += temporalScore * 0.4;

    // ì„¸ì…˜ ì§€ì†ì„± ë¶„ì„
    const { data: sessions } = await supabase
      .from('user_behaviors')
      .select('session_id, timestamp')
      .eq('user_id', userId)
      .order('timestamp', { ascending: false })
      .limit(50);

    if (sessions) {
      const sessionDurations = this.calculateSessionDurations(sessions);
      const avgDuration = sessionDurations.reduce((sum, dur) => sum + dur, 0) / sessionDurations.length;
      
      // ë§¤ìš° ì§§ê±°ë‚˜ ë§¤ìš° ê¸´ ì„¸ì…˜ì€ ì˜ì‹¬ìŠ¤ëŸ¬ì›€
      if (avgDuration < 10 || avgDuration > 7200) { // 10ì´ˆ ë¯¸ë§Œ ë˜ëŠ” 2ì‹œê°„ ì´ˆê³¼
        botScore += 0.3;
      }
    }

    return Math.min(1.0, botScore);
  }

  private static calculateSessionDurations(sessions: any[]): number[] {
    const sessionMap = new Map();
    
    sessions.forEach(session => {
      const sessionId = session.session_id;
      if (!sessionMap.has(sessionId)) {
        sessionMap.set(sessionId, []);
      }
      sessionMap.get(sessionId).push(new Date(session.timestamp).getTime());
    });

    return Array.from(sessionMap.values()).map(timestamps => {
      timestamps.sort();
      return (Math.max(...timestamps) - Math.min(...timestamps)) / 1000; // ì´ˆ ë‹¨ìœ„
    });
  }
}

/**
 * ğŸ¯ 3ë‹¨ê³„: ì½˜í…ì¸  ìœ ì‚¬ë„ ë° í’ˆì§ˆ ë¶„ì„
 */
class ContentQualityEngine {
  /**
   * ì½˜í…ì¸  ìœ ì‚¬ë„ ê³„ì‚° (Jaccard similarity + Semantic similarity)
   */
  static async calculateSimilarity(newContent: string, userId: string): Promise<number> {
    // ì‚¬ìš©ìì˜ ìµœê·¼ ì½˜í…ì¸  ì¡°íšŒ
    const { data: recentContent } = await supabase
      .from('user_behaviors')
      .select('behavior_data')
      .eq('user_id', userId)
      .eq('behavior_type', 'upload')
      .order('timestamp', { ascending: false })
      .limit(20);

    if (!recentContent || recentContent.length === 0) return 0;

    let maxSimilarity = 0;
    const newTokens = new Set(this.tokenize(newContent));

    for (const content of recentContent) {
      const existingText = content.behavior_data?.description || content.behavior_data?.title || '';
      const existingTokens = new Set(this.tokenize(existingText));
      
      // Jaccard Similarity
      const intersection = new Set([...newTokens].filter(token => existingTokens.has(token)));
      const union = new Set([...newTokens, ...existingTokens]);
      const similarity = intersection.size / union.size;
      
      maxSimilarity = Math.max(maxSimilarity, similarity);
    }

    return maxSimilarity;
  }

  /**
   * ë…ì„± ì½˜í…ì¸  íƒì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± + í–¥í›„ ML ëª¨ë¸ ì ìš© ê°€ëŠ¥)
   */
  static calculateToxicityScore(text: string): number {
    const toxicPatterns = [
      // ìš•ì„¤/ë¹„ë°© íŒ¨í„´
      /(\b(fuck|shit|damn|bitch|asshole)\b)/gi,
      // í˜ì˜¤ í‘œí˜„
      /(hate|kill|die|stupid|idiot)/gi,
      // ìŠ¤íŒ¸ íŒ¨í„´
      /(buy now|click here|free money|get rich)/gi,
      // ê³¼ë„í•œ ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì
      /((.)\2{4,})/g,
      // ì „í™”ë²ˆí˜¸/ì´ë©”ì¼ íŒ¨í„´
      /(\d{3}-\d{3,4}-\d{4}|[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,})/gi
    ];

    let toxicityScore = 0;
    const textLength = text.length;

    toxicPatterns.forEach(pattern => {
      const matches = text.match(pattern);
      if (matches) {
        toxicityScore += matches.length * 0.2;
      }
    });

    // ê³¼ë„í•œ ëŒ€ë¬¸ì ì‚¬ìš©
    const upperCaseRatio = (text.match(/[A-Z]/g) || []).length / textLength;
    if (upperCaseRatio > 0.7 && textLength > 10) {
      toxicityScore += 0.3;
    }

    // ë°˜ë³µ ë¬¸ì/ë‹¨ì–´
    const repetitionRatio = this.calculateRepetitionRatio(text);
    if (repetitionRatio > 0.5) {
      toxicityScore += 0.4;
    }

    return Math.min(1.0, toxicityScore);
  }

  private static tokenize(text: string): string[] {
    return text
      .toLowerCase()
      .replace(/[^\w\sê°€-í£]/g, ' ')
      .split(/\s+/)
      .filter(token => token.length > 1);
  }

  private static calculateRepetitionRatio(text: string): number {
    const words = this.tokenize(text);
    const uniqueWords = new Set(words);
    return words.length > 0 ? 1 - (uniqueWords.size / words.length) : 0;
  }
}

/**
 * ğŸ¯ 4ë‹¨ê³„: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì¢…í•© ë¶„ì„ ì—”ì§„
 */
class MLSpamClassifier {
  /**
   * ì¢…í•© ìŠ¤íŒ¸ ì ìˆ˜ ê³„ì‚° (ì•™ìƒë¸” ë°©ë²•)
   */
  static async calculateOverallSpamScore(
    contentSimilarity: number,
    behaviorScore: number,
    floodingScore: number,
    toxicityScore: number,
    botProbability: number
  ): Promise<number> {
    // ê°€ì¤‘ì¹˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” A/B í…ŒìŠ¤íŠ¸ë¡œ ìµœì í™”)
    const weights = {
      content: 0.25,
      behavior: 0.20,
      flooding: 0.30,
      toxicity: 0.15,
      bot: 0.10
    };

    const weightedScore = 
      contentSimilarity * weights.content +
      behaviorScore * weights.behavior +
      floodingScore * weights.flooding +
      toxicityScore * weights.toxicity +
      botProbability * weights.bot;

    // ë¹„ì„ í˜• ë³€í™˜ìœ¼ë¡œ ê·¹ê°’ ê°•í™”
    return Math.pow(weightedScore, 1.2);
  }

  /**
   * ìœ„í—˜ë„ ë ˆë²¨ íŒì •
   */
  static assessRiskLevel(spamScore: number): SpamDetectionResult['riskLevel'] {
    if (spamScore >= 0.8) return 'CRITICAL';
    if (spamScore >= 0.6) return 'HIGH';
    if (spamScore >= 0.3) return 'MEDIUM';
    return 'LOW';
  }

  /**
   * ì¶”ì²œ ì¡°ì¹˜ ê²°ì •
   */
  static recommendAction(
    spamScore: number, 
    userHistory: { warningCount: number; tempBanCount: number }
  ): SpamDetectionResult['recommendedAction'] {
    // ì´ˆë²” vs ì¬ë²” ê³ ë ¤
    const historyMultiplier = 1 + (userHistory.warningCount * 0.1) + (userHistory.tempBanCount * 0.3);
    const adjustedScore = Math.min(1.0, spamScore * historyMultiplier);

    if (adjustedScore >= 0.9) return 'PERMANENT_BAN';
    if (adjustedScore >= 0.7) return 'TEMP_BAN';
    if (adjustedScore >= 0.4) return 'SHADOW_BAN';
    return 'ALLOW';
  }
}

/**
 * ğŸ¯ 5ë‹¨ê³„: ë©”ì¸ ìŠ¤íŒ¸ íƒì§€ ì„œë¹„ìŠ¤
 */
export class AdvancedSpamDetectionService {
  /**
   * ì¢…í•© ìŠ¤íŒ¸ íƒì§€ ë¶„ì„
   */
  static async analyzeContent(
    userId: string,
    contentId: string,
    contentType: 'artwork' | 'comment' | 'message',
    content: {
      text?: string;
      imageUrl?: string;
      metadata?: any;
    },
    context: {
      userAgent?: string;
      sessionId?: string;
      deviceFingerprint?: string;
    } = {}
  ): Promise<SpamDetectionResult> {
    try {
      console.log('ğŸ” ê³ ê¸‰ ìŠ¤íŒ¸ íƒì§€ ë¶„ì„ ì‹œì‘:', { userId, contentId, contentType });

      // ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
      const [
        contentSimilarity,
        behaviorAnomalyScore,
        floodingScore,
        toxicityScore,
        botProbability,
        userHistory
      ] = await Promise.all([
        content.text ? ContentQualityEngine.calculateSimilarity(content.text, userId) : 0,
        BehaviorAnalysisEngine.detectTemporalAnomalies(userId),
        BehaviorAnalysisEngine.calculateFloodingScore(userId),
        content.text ? ContentQualityEngine.calculateToxicityScore(content.text) : 0,
        BehaviorAnalysisEngine.calculateBotProbability(userId, context.userAgent || '', context.deviceFingerprint || ''),
        this.getUserModerationHistory(userId)
      ]);

      // ì¢…í•© ìŠ¤íŒ¸ ì ìˆ˜ ê³„ì‚°
      const overallSpamScore = await MLSpamClassifier.calculateOverallSpamScore(
        contentSimilarity,
        behaviorAnomalyScore,
        floodingScore,
        toxicityScore,
        botProbability
      );

      const riskLevel = MLSpamClassifier.assessRiskLevel(overallSpamScore);
      const recommendedAction = MLSpamClassifier.recommendAction(overallSpamScore, userHistory);

      // ìŠ¤íŒ¸ ìœ í˜• ì‹ë³„
      const spamTypes = this.identifySpamTypes({
        contentSimilarity,
        behaviorAnomalyScore,
        floodingScore,
        toxicityScore,
        botProbability
      });

      const result: SpamDetectionResult = {
        isSpam: overallSpamScore > 0.3,
        confidence: Math.min(1.0, overallSpamScore * 1.2),
        spamType: spamTypes,
        riskLevel,
        recommendedAction,
        details: {
          contentSimilarity,
          behaviorAnomalyScore,
          floodingScore,
          toxicityScore,
          botProbability
        }
      };

      // ë¶„ì„ ê²°ê³¼ ì €ì¥
      await this.saveDetectionResult(userId, contentId, contentType, result, content);

      console.log('âœ… ìŠ¤íŒ¸ íƒì§€ ë¶„ì„ ì™„ë£Œ:', {
        isSpam: result.isSpam,
        confidence: result.confidence,
        riskLevel: result.riskLevel
      });

      return result;

    } catch (error) {
      console.error('ğŸ’¥ ìŠ¤íŒ¸ íƒì§€ ë¶„ì„ ì‹¤íŒ¨:', error);
      
      // ì•ˆì „ì¥ì¹˜: ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
      return {
        isSpam: false,
        confidence: 0,
        spamType: ['analysis_failed'],
        riskLevel: 'LOW',
        recommendedAction: 'ALLOW',
        details: {
          contentSimilarity: 0,
          behaviorAnomalyScore: 0,
          floodingScore: 0,
          toxicityScore: 0,
          botProbability: 0
        }
      };
    }
  }

  /**
   * ì‹¤ì‹œê°„ ìë™ ì¡°ì¹˜ ì‹¤í–‰
   */
  static async executeAutoModeration(userId: string, result: SpamDetectionResult): Promise<boolean> {
    if (result.recommendedAction === 'ALLOW') return true;

    try {
      const moderationData = {
        user_id: userId,
        action_type: this.mapActionToDBType(result.recommendedAction),
        reason: `Automated spam detection: ${result.spamType.join(', ')}`,
        evidence: {
          spam_score: result.confidence,
          risk_level: result.riskLevel,
          detection_details: result.details,
          detection_time: new Date().toISOString()
        },
        is_automated: true,
        automation_confidence: result.confidence,
        starts_at: new Date().toISOString(),
        expires_at: this.calculateExpiryTime(result.recommendedAction)
      };

      const { error } = await supabase
        .from('user_moderation')
        .insert(moderationData);

      if (error) throw error;

      console.log(`ğŸ”¨ ìë™ ì¡°ì¹˜ ì‹¤í–‰: ${result.recommendedAction} for user ${userId}`);
      return true;

    } catch (error) {
      console.error('ğŸ’¥ ìë™ ì¡°ì¹˜ ì‹¤í–‰ ì‹¤íŒ¨:', error);
      return false;
    }
  }

  /**
   * ì‚¬ìš©ì ì œì¬ ì´ë ¥ ì¡°íšŒ
   */
  private static async getUserModerationHistory(userId: string): Promise<{ warningCount: number; tempBanCount: number }> {
    const { data: history } = await supabase
      .from('user_moderation')
      .select('action_type')
      .eq('user_id', userId)
      .eq('is_active', true);

    return {
      warningCount: history?.filter(h => h.action_type === 'warning').length || 0,
      tempBanCount: history?.filter(h => h.action_type === 'temp_ban').length || 0
    };
  }

  /**
   * ìŠ¤íŒ¸ ìœ í˜• ì‹ë³„
   */
  private static identifySpamTypes(scores: any): string[] {
    const types = [];
    
    if (scores.contentSimilarity > 0.7) types.push('duplicate_content');
    if (scores.floodingScore > 0.6) types.push('flooding');
    if (scores.toxicityScore > 0.5) types.push('toxic_content');
    if (scores.botProbability > 0.7) types.push('bot_behavior');
    if (scores.behaviorAnomalyScore > 0.6) types.push('anomalous_pattern');
    
    return types.length > 0 ? types : ['general_spam'];
  }

  /**
   * íƒì§€ ê²°ê³¼ ì €ì¥
   */
  private static async saveDetectionResult(
    userId: string,
    contentId: string,
    contentType: string,
    result: SpamDetectionResult,
    content: any
  ): Promise<void> {
    const detectionData = {
      user_id: userId,
      content_id: contentId,
      content_type: contentType,
      content_hash: content.text ? ContentHashingEngine.generateTextHash(content.text) : null,
      spam_score: result.confidence,
      is_spam: result.isSpam,
      spam_type: result.spamType.join(','),
      detection_method: 'advanced_ml_ensemble',
      confidence: result.confidence
    };

    await supabase
      .from('spam_detection')
      .insert(detectionData);
  }

  /**
   * ì¡°ì¹˜ íƒ€ì… ë§¤í•‘
   */
  private static mapActionToDBType(action: SpamDetectionResult['recommendedAction']): string {
    const mapping = {
      'ALLOW': 'warning',
      'SHADOW_BAN': 'shadow_ban',
      'TEMP_BAN': 'temp_ban',
      'PERMANENT_BAN': 'permanent_ban'
    };
    return mapping[action] || 'warning';
  }

  /**
   * ì œì¬ ë§Œë£Œ ì‹œê°„ ê³„ì‚°
   */
  private static calculateExpiryTime(action: SpamDetectionResult['recommendedAction']): string {
    const now = new Date();
    switch (action) {
      case 'SHADOW_BAN':
        return new Date(now.getTime() + 24 * 60 * 60 * 1000).toISOString(); // 24ì‹œê°„
      case 'TEMP_BAN':
        return new Date(now.getTime() + 7 * 24 * 60 * 60 * 1000).toISOString(); // 7ì¼
      default:
        return new Date(now.getTime() + 365 * 24 * 60 * 60 * 1000).toISOString(); // 1ë…„ (ì‚¬ì‹¤ìƒ ì˜êµ¬)
    }
  }
}

// ì‚¬ìš©ì í–‰ë™ ì¶”ì ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ (ì„ì‹œ ë¹„í™œì„±í™”)
export const trackUserBehavior = async (
  userId: string,
  behaviorType: string,
  behaviorData: any = {},
  intensityScore: number = 1.0,
  sessionId?: string
): Promise<void> => {
  // ğŸ›‘ AI ì‹œìŠ¤í…œ ì„ì‹œ ë¹„í™œì„±í™” - ì¦‰ì‹œ ë°˜í™˜
  console.log('âš ï¸ AI ì‹œìŠ¤í…œ ë¹„í™œì„±í™”: trackUserBehavior í˜¸ì¶œ ë¬´ì‹œë¨');
  return;
  
  try {
    await supabase
      .from('user_behaviors')
      .insert({
        user_id: userId,
        behavior_type: behaviorType,
        behavior_data: behaviorData,
        intensity_score: intensityScore,
        session_id: sessionId || 'unknown',
        timestamp: new Date().toISOString()
      });
  } catch (error) {
    console.error('í–‰ë™ ì¶”ì  ì‹¤íŒ¨:', error);
  }
};
